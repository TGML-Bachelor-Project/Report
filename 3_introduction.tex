\section{Introduction}
\label{sec:Intro}

%This section introduces the project and the motivations behind it.
%It will lightly shed light on the technical background of the project, introducing machine learning on graph networks.
%A review of previous, related work is given, followed by stating and explaining the present research questions.
%Lastly, an outline of the contents of the project will be given.


\subsection{Motivation}
\label{sec:Intro:Motivation}
Many real world systems and problems can be represented as static graph networks, a representation that enables both a human and machine-interpretable understanding of the given system, further allowing for the use of various machine learning tasks such as link prediction 
%(Thomas N. Kipf and Max Welling. Variational Graph Auto-Encoders. 2016. arXiv: 1611.07308 [stat.ML], Aditya Grover and Jure Leskovec. Node2vec: Scalable Feature Learning for Networks. New York, NY, USA, 2016. doi: 10.1145/2939672.2939754. url: https://doi.org/10.1145/2939672.2939754., William L. Hamilton, Rex Ying, and Jure Leskovec. Representation Learning on Graphs: Methods and Applications. 2018. arXiv: 1709.05584 [cs.SI].)
.
Hence, in order to grasp the structure of a social network such as Facebook, a static graph network representation seems appropriate.
Yet, to fully grasp such a network system, the aspect of time is crucial.
In the case of Facebook, taking a static snapshot, ie. a static graph representation, will show us how it's users relate to each other for one instant, but next to nothing about how these users interact.
It is evident that to properly understand interactions between users, say how they make new connections and break old, the the graph network representation must be temporally dynamic.

This project is motivated by this fact, and works solely with networks that change over time, which can be represented as temporally dynamic graph networks (TDGNs).
Approaches to modelling TDGN's have in recent years proved successful in allowing for well performing machine learning task
%(Srijan Kumar, Xikun Zhang, and Jure Leskovec. Predicting Dynamic Em-bedding Trajectory in Temporal Interaction Networks. New York, NY,USA, 2019. doi: 10.1145/3292500.3330895. url: https://doi.org/10.1145/3292500.3330895., Rakshit Trivedi et al. DyRep: Learning Representations over Dynamic Graphs. 2019. url: https://openreview.net/forum?id=HyePrhR5KX., da Xu et al. Inductive representation learning on temporal graphs. 2020. url: https://openreview.net/forum?id=rJeW1yHYwH., Emanuele Rossi et al. Temporal Graph Networks for Deep Learning onDynamic Graphs. 2020. arXiv: 2006.10637 [cs.LG].)
, yet most lack explainability and the ability for human-level understanding.
This project is further motivated by this, and seeks to investigate modelling of TDGNs in a manner that enforces explainability. 

A very recent work was developed by Simon Tommerup et. al. \cite{Tommerup2021LearningNetworks}, models TDGN's with while enforcing explainability, by utilizing a simpler modelling approach based on Newtonian dynamics in the latent space.
The work though suffers in terms of information loss, ie. the amount of information is kept during modelling, and severely in terms of scalability.
On the basis of this Newtonian dynamics modelling approach, the fundamental goal with this project is to expand it's modelling capabilities, which allow for explainability and visualization, while improving scalability, making them suitable for use on larger, more complex TDGN data.



\subsection{Dynamic Graph Network representations}
\label{sec:Intro:DynamicGraphNetworks}




\subsection{Related Work}
\label{sec:Intro:RelatedWork}
The work presented in this project is related to, and builds upon, many principles established in earlier work.

The first work to establish the use of Latent Space as a means of modelling graph networks was Hoff P et. al. \cite{Hoff2002LatentAnalysis}, in which the concept of depicting a static graph as nodes in latent space, with their relative distances describing likelihood of interaction, was introduced.

In "Dynamic Social Network Analysis using Latent Space Models" by Sarkar and Moore \cite{Sarkar2005DynamicModels}, the latent space approach was expanded to dynamic graph networks.
%, and the Discrete Diffusion model introduced.
%The Discrete Diffusion model entails 
The approach developed by Sarkar and Moore serves as an early foundation for this project in terms of modelling TDGNs in an explainable manner.

Several publications have since Sarkar and Moore been made, all presenting latent modelling approaches for dynamic graph networks. 
Worth mentioning are DyRep, Rakshit Trivedi 2019 et. al. \cite{TrivediDYREP:GRAPHS}, which like this project uses a temporal point process loss, and TGN, Emanuele Rossi 2020 et. al. \cite{RossiTEMPORALGRAPHS}, which .
 
One of the most recent works, on which this project builds, was developed by Simon Tommerup et. al. \cite{Tommerup2021LearningNetworks}.
His work seeks to simplify the latent space approach, by utilizing the simple Poisson point process, and enforces explainability by utilizing Newtonian dynamics in the latent space.




\subsubsection{Research Questions} 
\label{sec:Intro:ResearchQs}
The main scope, and essentially the overarching research question of this project lies in determining how a scalable and explainable model can be implemented for visualizing interactions in a temporal dynamic graph network.
In order to answer this question, three research questions have been put forth.
The first of these is
\\\\
\hspace*{5mm} 1. How well can dynamic networks be modelled using a SCVM representation in latent Euclidean space, with stepwise event computation?
%\hspace*{5mm} 1. How well can TDGNs be modelled using a velocity-dependant representation in latent Euclidean space, with stepwise event computation, and how well can it model the evolution of the TDGNs?
\\
This research question will be explored in two steps:
First, the constant velocity model will be implemented, based in the state of the art approach by Simon Tommerup et. al., as it serves as the foundation of modelling TDGN's through learning a starting position and a constant velocity vector for each node.
Then the approach will be expanded with stepwise computation, meaning each node is attributed several velocity vectors in order to better reflect changes over time.
\\
Evaluating the modelling ability of these approaches involves testing how well the model is able to infer correct node interactions from unseen data, and it's ability to map data of entire node pairs when these are unseen during testing.
\\\\
\hspace*{5mm} 2. To what extend can the model be implemented in a scalable manner?
\\
The current approaches to modelling TDGN's are heavily limited in terms of computational cost and are unable to model larger networks on accessible hardware in a timely fashion.
A scalable model will be able to model TDGN's in a way that is much more computationally efficient than prior approaches, and allow for the modelling of larger, more complex networks.
\\
This research question will be answered by implementing the computations of the proposed model in a parallelizable manner, and evaluated based on the improvement in efficiency over conventional, non-parralized computation approach.
\\\\
\hspace*{5mm} 3. How can the model be visualized to enforce explainability?
\\
A key advantage to the modelling approach using latent space representations is it's explainability, ie. that it is interpretable by a human observer.
Expanding to a stepwise constant velocity modelling approach should allow for a more detailed interpretation of the complex behavior that a TDGN might have over it's temporal duration.
\\
In order to answer this research question, visualizations of the TDGN as learned by the modelling approach will be evaluated based on interpretability. 



\subsection{Project Outline}
\label{sec:Intro:ThesisOutline}
The project consist of 4 more sections.

Section 2, the methodology section, provides a thorough technical explanation of the work presented in this project.
The first half of this section explains the theoretical aspects underlying the proposed model.
This part also describes the way synthetic data is generated for testing the model.


The second half fleshes out the proposed model, giving a detailed description of the optimization taking place within it, and hence the learning of the stepwise, dynamical representation that the model produces.
An explanation of the scalability aspects is further given, as this too defines how the model functions, in a parallizable manner.
The sections ends with an outline of details relating to reproducibility of the present work, enabling for future work.

Section 3 presents and analyzes the results which relate to answering the three research questions.

Section 4 discusses the present work, and gives some context as to how the work may be utilized in real-life use-cases.

Section 5 is the conclusion, summarizing the key findings presented in the project, hence wrapping up.
\\\\
All code for the project is publicly available at:
\\
\href{https://github.com/TGML-Bachelor-Project/TGML}{\textcolor{blue}{https://github.com/TGML-Bachelor-Project/TGML}}





