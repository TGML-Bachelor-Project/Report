\subsection{Model Implementation}
\label{sec:Method:ModelImplementation}
The proposed model takes care of attributing learned parameters, $z0$, $v0$ and $\beta$ with a loss on which it can be optimized.
The optimization itself is explained in this subsection.


\subsubsection{Learning}
\label{sec:Method:ModelImplementation:Learning}
The model training is implemented using the \hyperlink{https://pytorch-ignite.ai/}{Pytorch-Ignite} framework.



\subsubsection{Adam Optimizer}
\label{sec:Method:ModelImplementation:Adam}
Training the models in this project is done using the Adam optimization algorithm, a form of stochastic gradient descent.
A gradient descent algorithm seeks to optimize learnable parameters by stepping towars the direction of the negative gra
Talk about learning rate.


\subsubsection{Position Correction}
\label{sec:Method:ModelImplementation:PositionCorrection}




