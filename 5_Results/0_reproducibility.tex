\subsection{Training Setup and Hyperparameters}
\label{sec:Method:Reproducibility:TrainingSetup}
The proposed model, as well as baseline models, are governed by several hyperparameter options that naturally changes result-outcomes. 
\\\\
Training the models in this project is done using the Adam optimization algorithm, short for Adaptive Moment Estimation, which is a form of stochastic gradient descent \cite{Kingma2014Adam:Optimization}.
% A gradient descent algorithm seeks to optimize learnable parameters by stepping towards the direction of the negative gradient of the loss function with respect to these parameters.
% This way, the model will eventually find the parameters that, globally or locally, minimize the loss function.
% What makes the Adam optimization a stochastic gradient descent is the fact that the gradient is a stochastic approximation of the gradient based on the given batch of training data.
The most important hyperparameter in regards to the Adam optimization algorithm is the learning rate, usually denoted by $\alpha$, which determines the step size that is taken in the direction of the negative gradient. 
%As Adam builds upon the RMSProp optimizer, short for Root Mean Square Propagation, this learning rate is adapted for each learnable parameter, and hence slightly changing.

For this project, a learning rate of $lr = 0.025$ was found to be a good fit as it converges relatively quickly while not overshooting for most trainings.
Other hyperparameters of the training setup is Pytorch default.

Note that all random number generators in this project, belonging to Python, Numpy and Torch, are set to $seed = 1$.

While the number of Epochs needed by the SCVM for properly learning different dynamic networks, the proposed model could in most cases train for 5000 epochs in at most a couple of hours.
5000 epochs was enough for all trainings to seemingly converge.

