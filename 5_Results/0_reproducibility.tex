\subsection{Training Setup (Hyperparameters)}
\label{sec:Method:Reproducibility:TrainingSetup}
The proposed model, as well as baseline models, are governed by several hyperparameter options that naturally changes result-outcomes. 

The most important hyperparameter, as mentioned under section \ref{sec:Method:Learning:Adam} about the Adam Optimizer, is the learning rate associated with this optimizer. 
For this project, a learning rate of $lr = 0.025$ was found to be a good fit.
Note that all random number generators in this project, belonging to Python, Numpy and Torch, are set to $seed = 1$.
The number of Epochs used for training varies for the different datasets, and as such are listed below:

\begin{table}[H]
\centering
\begin{tabular}{|l|cccc|}
\hline
Dataset             & Num. Epochs & Size $n$ & $t_{max}$ & Learning Rate \\ \hline
Synthetic 1         & 5000          & 79,424   & 10        & 0.025       \\
Synthetic 2         & 5000          & XX       & 50        & 0.025         \\
Synthetic 3         & 5000          & XX       & 15        & 0.025         \\
Real 1: Resistance  & 5000          & XX       & XX        & 0.025         \\
Real 2: EU          & 5000          & XX       & XX        & 0.025         \\
Real 3: Lyon        & 5000          & XX       & XX        & 0.025         \\ \hline
\end{tabular}
\caption{Dataset details: Number of epochs, size of dataset n, max time $t_{max}$}
\label{tab:DatasetDetails}
\end{table}