\subsection{Future Work and Improvements}
\label{sec:Discussion:FutureWork}
The work of this project takes a big step towards utilizing the latent space modelling approach as a detailed, scalable means of modelling dynamic networks in an explainable way.
In regards to the implemented model, the discussion points regarding the results discuss potential improvements that could be made to the already implemented components that make up the SCVM and visualisation. 
In order to fully realize the potential of the modelling approach though, some further, new components could advantageously be added on top of the work presented in this project.


\subsubsection{Memory Limitations}
\label{sec:Discussion:FutureWork:MemoryLimitations}
A problem which was met when running larger dynamic networks, of $N = 100$ nodes and more, was the memory allocation requirement.
With an increased number of unique nodes and an increased number of steps, the amount of memory needed to be allocated expands fast due to the tensor having dimensions of $N \times 2 \times S$, ie. number of nodes times number of dimensions (2) times number of steps. 
Further, the number of unique nodes is an even greater limiting factor, since the node pairwise computations are performed on $S$ $N \times N$ matrices, hence growing exponentially with $N$.
An improvement to be made for the vectorized setup consist first and foremost in handling this memory requirement. 
The code underlying the proposed model is not particularly optimized in terms of reducing memory requirements, as the focus has mainly been on optimizing the scalability in regards to run time. 
\\
One way the memory requirements could potentially be reduced though, would be by splitting up the computation of node interaction intensities through batching of the nodes. 
This is in theory easily done, as the computations of each node batch intensity can be computed and summed, thereby keeping an upper limit on how many nodes are included in each computation. 
\\
Another approach could be to not directly do node-wise batching, but instead (or also) use multiple GPUs. 
This approach was considered during the project, but due to large queue times on the DTU HPC servers when requesting multiple GPUs, did not come to fruition. 
This should however quite easily be done by using the Pytorch DataParallel \cite{DataParallelDocumentation} class and specifying multiple device IDs. 
\\
Yet another way of improving memory utilization would be to check the model consistently to ensure objects sent to the GPU device are detached optimally when no longer used.
Further, checking and converting data types that can be reduced in size, e.g. float64 to float32, could be explored.
Also, exploring the possibility of using sparse data representations \cite{Torch.sparseDocumentation} for large tensors would be interesting.


\subsubsection{Bias Term $\beta$}
\label{sec:Discussion:Results:BiasTerm}
The use of a dataset-wide bias term is possibly insufficient in terms of correctly representing the different nodes of the dynamic network. 
This fact is of course not true for the synthetic dataset results, as the data of these sets are all generated from a single beta value, but for the real datasets it might very well be the case.
In a given network, a very plausible scenario may be that some entities are less active than others, and with the current setup, the SCVM fits a $\beta$ value that reflects greater background intensity than these entities entail.
In order to compensate for this difference in average intensity, the SCVM will place these less active entities further away from the more active entities, hence neglecting the proportions of each individual node's average interaction intensity.
A fitting expansion of the model would be the implementation of node-specific bias terms, with which every node is attributed its own beta value, and as such can be spatially modelled in latent space while accounting for the node's average interaction intensity. 
\\
For the proposed SCVM, the fact that the different steps are governed by the same bias term may also be a limitation. 
The average intensities for each step may vary a lot, just as with the individual nodes.
Hence, another expansion in relation to the bias term would be to have step-specific bias terms, each governing the background intensity of the given step.
\\
Finally a combination of the node-specific bias term and the step-specific would then be the node-step-specific bias term, learning a beta parameter for each node for each step.


\subsubsection{Alternative Training Setup}
\label{sec:Discussion:Results:AlternativeTrainingSetups}
For this project, the training was carried out using the Pytorch Ignite framework, and during all training epochs, the parameters $\beta$, $z0$ and $v0$ were all optimized at the same time.
This training setup was deemed appropriate for the SCVM, as it was able to converge well given a training length of 5000 epochs, which could be completed in a feasible time frame.
The length of the training was made possible by the speedup in computation, a result of improving scalability through vectorization, as well as the fact that the datasets used in this project were of a certain size.
\\
When training on the Lyon dataset, which has 237 nodes and 3.5 million interactions, fitting 249 steps, the training time was around 14 hours when running on an NVIDIA GTX 1650 GPU.
For \textit{even} larger dynamic networks, perhaps with several thousand nodes and several millions of interactions, the training of the SCVM will likely take days at least, which in many cases poses as infeasible. 
In order to enable training of such large dynamic networks in a manageable time frame, convergence of model parameters should ideally happen on the basis of relatively fewer epochs.
One way to potentially realize this would be to run a sequential training setup, in which some parameters are initialized and learned before others.
For the SCVM, this could mean initializing only the position parameter $\textbf{Z}$ first, and for the first proportion of the training, say a third, only seek to optimize the model by changing the initial positions.
Then, the model would initialize the velocities for all nodes $\textbf{V}$, and for the middle third of training fit only the positions and velocities. 
Lastly, the SCVM it would train the model while also fitting the $\beta$ parameter.
\\
Another solution, as presented for solving the memory limitations, would be to use multiple GPUs either through Pytorch or scale up to multiple clusters through other frameworks like Apache Spark\cite{ApacheAnalytics}.
Distributing the computations would enable a greater use of parallelization, likely improving run times by a significant amount.

\subsubsection{Optimizing Visualizations}
For creating the animations in this project, a simple dataframe of the learned parameters together with an animated plotly \cite{PlotlyPlotly} scatter plot is used. 
However, the animation process could be made far more scalable by using other available frameworks for graph creation and visualization, and allow for higher time-resolution animations to be created. 
One such framework is Graphviz \cite{Graphviz}, which is optimized specifically for graph visualizations, and could advantageously be used in future work. 
Using Graphviz should first of all make the visualizations scale to even larger amounts of nodes higher time-resolutions. 
Also, edges are an integrated part of Graphviz, and the framework would therefore allow for easy visualizations of edges between nodes, when they interact according to the input data. As mentioned in the discussion about the results of research question 3, this could increase the interpretability of the visualizations even further. 